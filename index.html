
<!doctype html>
<html>

<head>
  <title>Jin Liu - Homepage</title>
  <meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1">
  <link href="css/frame.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/controls.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/custom.css" media="screen" rel="stylesheet" type="text/css" />
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300,700' rel='stylesheet' type='text/css'>
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="js/menu.js"></script>
  <style>
    .menu-index {
      color: rgb(255, 255, 255) !important;
      opacity: 1 !important;
      font-weight: 700 !important;
    }
  </style>
</head>

<body>
  <div class="menu-container"></div>
  <div class="content-container">
    <div class="content">
      <div class="content-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column">
            <img src="images/JinLiu_avartar.jpg" height="200">
          </div>
          <div class="flex-item flex-column">
            <p class="text" style="font-size:19px">
              <span><strong>Jin Liu (刘锦)</strong></span><br />
              <span>Ph.D. student</span><br />
              <span><a href="http://www.iie.ac.cn/">Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China</a></span><br />
            </p>
            <p class="text text-no-margin">
                <span>Email: <a href="mailto:liujin@iie.ac.cn">liujin [at] iie.ac.cn</a></span><br />
              <span>
                  [<a href='https://scholar.google.com/citations?user=DBz5oyAAAAAJ'>Google Scholar</a>]&nbsp;
                  [<a href='https://github.com/DaddyJin'>GitHub</a>]&nbsp;
              </span>
              <br />
            </p>
          </div>
        </div>


        <h2 id="AboutMe" class="add-top-margin">About Me</h2>
        <hr>
            <div>
                I am currently a final-year Ph.D. student at <a href="http://www.iie.ac.cn/">Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China.
                I am affiliated with the Third Laboratory in IIE,CAS,
                supervised by Prof. <a href='https://people.ucas.ac.cn/~hjz'>Jizhong Han</a>.
                I received my bachelor’s degree from <a href="https://www.bjtu.edu.cn/">Beijing Jiaotong University</a> in July 2019.
                <br>
                My research interests lie in computer vision, generative models, image and video synthesis, especially in face reenactment and talking head generation.
            </div>

        <h2 id="News" class="add-top-margin">News</h2>
        <hr>
            <div>
                <li>[03/2023] Two papers are accepted to ICME 2023.</li>
                <li>[02/2023] One paper is accepted to ICASSP 2023.</li>
                <li>[07/2022] One survey paper is accepted to Journal of Image and Graphics.</li>
              
                <li>[07/2018] Join <a href='http://www.iie.ac.cn/'>IIE, CAS</a>, NTU.</li>

<!--                </details>-->
            </div>

        <h2 id="Experience" class="add-top-margin">Experience</h2>
        <hr>
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <img src="images/adobe.png" width="190">
          </div>
          <div class="flex-item flex-item-stretch-4 flex-column">
            <p class="text">
                <a class="highlight-text" href="https://research.adobe.com/">Adobe Research</a>. San Jose, CA, USA<br>
                <b>Research Scientist Intern</b><br>
                <i>May 2023 - Aug. 2023</i><br>
                Collaborators:
                <a href="https://krsingh.cs.ucdavis.edu/">Krishna Kumar Singh</a>,
                <a href="http://richzhang.github.io/">Richard Zhang</a>,
                <a href="https://yijunmaverick.github.io/">Yijun Li</a>,
                <a href="https://scholar.google.com/citations?user=jN2Y51YAAAAJ">Jingwan (Cynthia) Lu</a>
            </p>
          </div>
        </div>
        <br>
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <img src="images/meta.gif" width="190">
          </div>
          <div class="flex-item flex-item-stretch-4 flex-column">
            <p class="text">
                <a class="highlight-text" href="https://about.facebook.com/">Meta Platforms, Inc.</a> Redmond, WA, USA<br>
                <b>Research Scientist Intern</b> - CV, AI and Machine Perception (PhD)<br>
                <i>Aug. 2022 - Jan. 2023</i><br>
                Collaborators:
                <a href="https://scholar.google.com/citations?user=tXJDPJAAAAAJ">Chen Kong</a>,
                <a href="https://www.linkedin.com/in/justin-d-theiss/">Justin Theiss</a>,
                <a href="https://aayushp.github.io/">Aayush Prakash</a>,
<!--                <br>-->
<!--                <a href="https://scholar.google.com/citations?user=xSqC83MAAAAJ">David Molyneaux</a>,-->
                <a href="https://scholar.google.co.uk/citations?user=MhowvPkAAAAJ">Richard Newcombe</a>
            </p>
          </div>
        </div>
        <br>
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <img src="images/sensetime.png" width="190">
          </div>
          <div class="flex-item flex-item-stretch-4 flex-column">
            <p class="text">
                <a class="highlight-text" href="https://www.sensetime.com/en">SenseTime Research</a>. Beijing, China<br>
                <b>Computer Vision Research Intern</b><br>
                <i>Jan. 2019 - June 2019</i><br>
                Collaborators:
                Mingyang Huang,
                <a href="https://scholar.google.com/citations?user=4m061tYAAAAJ">Chunxiao Liu</a>,
                <a href="https://scholar.google.com/citations?user=mwsxrm4AAAAJ">Jianping Shi</a>
            </p>
          </div>
        </div>

        <h2 id="Publications" class="add-top-margin">Publications</h2>
        <hr>
<!--        <table>-->
<!--        <tr>-->
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <img src="images/GPUNIT_TPAMI2.jpg" width="300">
          </div>
          <div class="flex-item flex-item-stretch-4 flex-column">
            <p class="text">
                <a class="highlight-text" href="https://arxiv.org/abs/2306.04636">GP-UNIT: Generative Prior for Versatile Unsupervised Image-to-Image Translation</a><br>
                Shuai Yang, <b>Liming Jiang</b>, Ziwei Liu, Chen Change Loy<br>
                <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2023</i><br>
                [<a href="https://arxiv.org/abs/2306.04636">Paper</a>]&nbsp;
                [<a href="https://www.mmlab-ntu.com/project/gpunit/">Project Page</a>]&nbsp;
                [<a href="https://github.com/williamyang1991/GP-UNIT">GitHub (Code)</a>&nbsp;<iframe src="https://img.shields.io/github/stars/williamyang1991/GP-UNIT?style=social" frameborder="0" vertical-align="middle" scrolling="0" width="90px" height="20px"></iframe>]
            </p>
          </div>
        </div>
        <br>
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
<!--            <video muted autoplay="autoplay" loop="loop" controls src="images/vtoonify_overview.mp4" width="300"></video>-->
              <img src="images/StyleGANEX.jpg" width="300">
          </div>
          <div class="flex-item flex-item-stretch-4 flex-column">
            <p class="text">
                <a class="highlight-text" href="https://arxiv.org/abs/2303.06146">StyleGANEX: StyleGAN-Based Manipulation Beyond Cropped Aligned Faces</a><br>
                Shuai Yang, <b>Liming Jiang</b>, Ziwei Liu, Chen Change Loy<br>
                <i>arXiv preprint, 2023</i><br>
                [<a href="https://arxiv.org/abs/2303.06146">Paper</a>]&nbsp;
                [<a href="https://www.mmlab-ntu.com/project/styleganex/">Project Page</a>]&nbsp;
                [<a href="https://github.com/williamyang1991/StyleGANEX">GitHub (Code)</a>&nbsp;<iframe src="https://img.shields.io/github/stars/williamyang1991/StyleGANEX?style=social" frameborder="0" vertical-align="middle" scrolling="0" width="90px" height="20px"></iframe>]
            </p>
          </div>
        </div>
        <br>
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <img src="images/celebvtext.gif" width="300">
          </div>
          <div class="flex-item flex-item-stretch-4 flex-column">
            <p class="text">
                <a class="highlight-text" href="https://arxiv.org/abs/2303.14717">CelebV-Text: A Large-Scale Facial Text-Video Dataset</a><br>
                Jianhui Yu*, Hao Zhu*, <b>Liming Jiang</b>, Chen Change Loy, Weidong Cai, Wayne Wu<br>
                <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023</i><br>
                [<a href="https://arxiv.org/abs/2303.14717">Paper</a>]&nbsp;
                [<a href="https://celebv-text.github.io/">Project Page</a>]&nbsp;
                [<a href="https://github.com/CelebV-Text/CelebV-Text">GitHub (Dataset & Code)</a>&nbsp;<iframe src="https://img.shields.io/github/stars/CelebV-Text/CelebV-Text?style=social" frameborder="0" vertical-align="middle" scrolling="0" width="90px" height="20px"></iframe>]
            </p>
          </div>
        </div>
        <br>
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
<!--            <video muted autoplay="autoplay" loop="loop" controls src="images/vtoonify_overview.mp4" width="300"></video>-->
              <img src="images/VToonify.gif" width="300">
          </div>
          <div class="flex-item flex-item-stretch-4 flex-column">
            <p class="text">
                <a class="highlight-text" href="https://arxiv.org/abs/2209.11224">VToonify: Controllable High-Resolution Portrait Video Style Transfer</a><br>
                Shuai Yang, <b>Liming Jiang</b>, Ziwei Liu, Chen Change Loy<br>
                <i>ACM Transactions on Graphics (SIGGRAPH Asia), 2022</i><br>
                <i><font color="#F00000" size="2">(Journal track, selected as a cover image of the TOG issue)</font></i><br>
                [<a href="https://arxiv.org/abs/2209.11224">Paper</a>]&nbsp;
                [<a href="https://www.mmlab-ntu.com/project/vtoonify/">Project Page</a>]&nbsp;
                [<a href="https://github.com/williamyang1991/VToonify">GitHub (Code)</a>&nbsp;<iframe src="https://img.shields.io/github/stars/williamyang1991/VToonify?style=social" frameborder="0" vertical-align="middle" scrolling="0" width="90px" height="20px"></iframe>]
            </p>
          </div>
        </div>
        <br>
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <img src="images/CelebVHQ.jpg" width="300">
          </div>
          <div class="flex-item flex-item-stretch-4 flex-column">
            <p class="text">
                <a class="highlight-text" href="https://arxiv.org/abs/2207.12393">CelebV-HQ: A Large-Scale Video Facial Attributes Dataset</a><br>
                Hao Zhu*, Wayne Wu*, Wentao Zhu, <b>Liming Jiang</b>, Siwei Tang, Li Zhang, Ziwei Liu, Chen Change Loy<br>
                <i>European Conference on Computer Vision (ECCV), 2022</i><br>
                [<a href="https://arxiv.org/abs/2207.12393">Paper</a>]&nbsp;
                [<a href="https://celebv-hq.github.io/">Project Page</a>]&nbsp;
                [<a href="https://github.com/CelebV-HQ/CelebV-HQ">GitHub (Dataset & Code)</a>&nbsp;<iframe src="https://img.shields.io/github/stars/CelebV-HQ/CelebV-HQ?style=social" frameborder="0" vertical-align="middle" scrolling="0" width="90px" height="20px"></iframe>]
            </p>
          </div>
        </div>
        <br>
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <img src="images/DualStyleGAN.gif" width="300">
          </div>
          <div class="flex-item flex-item-stretch-4 flex-column">
            <p class="text">
                <a class="highlight-text" href="https://arxiv.org/abs/2203.13248">Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer</a><br>
                Shuai Yang, <b>Liming Jiang</b>, Ziwei Liu, Chen Change Loy<br>
                <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022</i><br>
                [<a href="https://arxiv.org/abs/2203.13248">Paper</a>]&nbsp;
                [<a href="https://www.mmlab-ntu.com/project/dualstylegan/">Project Page</a>]&nbsp;
                [<a href="https://github.com/williamyang1991/DualStyleGAN">GitHub (Code)</a>&nbsp;<iframe src="https://img.shields.io/github/stars/williamyang1991/DualStyleGAN?style=social" frameborder="0" vertical-align="middle" scrolling="0" width="90px" height="20px"></iframe>]
            </p>
          </div>
        </div>
        <br>
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <img src="images/GPUNIT.jpg" width="300">
          </div>
          <div class="flex-item flex-item-stretch-4 flex-column">
            <p class="text">
                <a class="highlight-text" href="https://arxiv.org/abs/2204.03641">Unsupervised Image-to-Image Translation with Generative Prior</a><br>
                Shuai Yang, <b>Liming Jiang</b>, Ziwei Liu, Chen Change Loy<br>
                <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022</i><br>
                [<a href="https://arxiv.org/abs/2204.03641">Paper</a>]&nbsp;
                [<a href="https://www.mmlab-ntu.com/project/gpunit/">Project Page</a>]&nbsp;
                [<a href="https://github.com/williamyang1991/GP-UNIT">GitHub (Code)</a>&nbsp;<iframe src="https://img.shields.io/github/stars/williamyang1991/GP-UNIT?style=social" frameborder="0" vertical-align="middle" scrolling="0" width="90px" height="20px"></iframe>]
            </p>
          </div>
        </div>
        <br>
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <img src="images/TransEditor.jpg" width="300">
          </div>
          <div class="flex-item flex-item-stretch-4 flex-column">
            <p class="text">
                <a class="highlight-text" href="https://arxiv.org/abs/2203.17266">TransEditor: Transformer-Based Dual-Space GAN for Highly Controllable Facial Editing</a><br>
                Yanbo Xu*, Yueqin Yin*, <b>Liming Jiang</b>, Qianyi Wu, Chengyao Zheng, Chen Change Loy, Bo Dai, Wayne Wu<br>
                <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022</i><br>
                [<a href="https://arxiv.org/abs/2203.17266">Paper</a>]&nbsp;
                [<a href="https://billyxyb.github.io/TransEditor/">Project Page</a>]&nbsp;
                [<a href="https://github.com/BillyXYB/TransEditor">GitHub (Code)</a>&nbsp;<iframe src="https://img.shields.io/github/stars/BillyXYB/TransEditor?style=social" frameborder="0" vertical-align="middle" scrolling="0" width="90px" height="20px"></iframe>]
            </p>
          </div>
        </div>
        <br>
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <img src="images/VDU_Occlusion.jpg" width="300">
          </div>
          <div class="flex-item flex-item-stretch-4 flex-column">
            <p class="text">
                <a class="highlight-text" href="https://arxiv.org/abs/2205.06218">Delving into High-Quality Synthetic Face Occlusion Segmentation Datasets</a><br>
                Kenny T. R. Voo, <b>Liming Jiang</b>, Chen Change Loy<br>
                <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2022</i><br>
                <i><font color="#F00000" size="2">(Best Paper Award in NTU AI Research Student Conference (ARSC), 2022)</font></i><br>
                [<a href="https://arxiv.org/abs/2205.06218">Paper</a>]&nbsp;
                [<a href="https://github.com/kennyvoo/face-occlusion-generation">GitHub (Dataset & Code)</a>&nbsp;<iframe src="https://img.shields.io/github/stars/kennyvoo/face-occlusion-generation?style=social" frameborder="0" vertical-align="middle" scrolling="0" width="90px" height="20px"></iframe>]
            </p>
          </div>
        </div>
        <br>
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <img src="images/APA.jpg" width="300">
          </div>
          <div class="flex-item flex-item-stretch-4 flex-column">
            <p class="text">
                <a class="highlight-text" href="https://arxiv.org/abs/2111.06849">Deceive D: Adaptive Pseudo Augmentation for GAN Training with Limited Data</a><br>
                <b>Liming Jiang</b>, Bo Dai, Wayne Wu, Chen Change Loy<br>
                <i>Conference on Neural Information Processing Systems (NeurIPS), 2021</i><br>
                [<a href="./projects/APA/resources/paper.pdf">Paper</a>]&nbsp;
                [<a href="https://www.mmlab-ntu.com/project/apa/index.html">Project Page</a>]&nbsp;
                [<a href="https://github.com/EndlessSora/DeceiveD">GitHub (Code)</a>&nbsp;<iframe src="https://img.shields.io/github/stars/EndlessSora/DeceiveD?style=social" frameborder="0" vertical-align="middle" scrolling="0" width="90px" height="20px"></iframe>]
            </p>
          </div>
        </div>
        <br>
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <img src="images/FFL.jpg" width="300" height="160">
          </div>
          <div class="flex-item flex-item-stretch-4 flex-column">
            <p class="text">
                <a class="highlight-text" href="https://arxiv.org/abs/2012.12821">Focal Frequency Loss for Image Reconstruction and Synthesis</a><br>
                <b>Liming Jiang</b>, Bo Dai, Wayne Wu, Chen Change Loy<br>
                <i>IEEE International Conference on Computer Vision (ICCV), 2021</i><br>
                [<a href="./projects/FFL/resources/paper.pdf">Paper</a>]&nbsp;
                [<a href="https://www.mmlab-ntu.com/project/ffl/index.html">Project Page</a>]&nbsp;
                [<a href="https://github.com/EndlessSora/focal-frequency-loss">GitHub (Code)</a>&nbsp;<iframe src="https://img.shields.io/github/stars/EndlessSora/focal-frequency-loss?style=social" frameborder="0" vertical-align="middle" scrolling="0" width="90px" height="20px"></iframe>]
            </p>
          </div>
        </div>
        <br>
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <img src="images/deeperforensics_chapter.jpg" width="300" height="98">
          </div>
          <div class="flex-item flex-item-stretch-4 flex-column">
            <p class="text">
                <a class="highlight-text" href="https://link.springer.com/chapter/10.1007/978-3-030-87664-7_14">DeepFakes Detection: The DeeperForensics Dataset and Challenge</a><br>
                <b>Liming Jiang</b>, Wayne Wu, Chen Qian, Chen Change Loy<br>
                <i>Book chapter. In Handbook of Digital Face Manipulation and Detection - From DeepFakes to Morphing Attacks, Springer, 2022</i><br>
                [<a href="https://link.springer.com/book/10.1007/978-3-030-87664-7">Book Link</a>]&nbsp;
                [<a href="https://link.springer.com/chapter/10.1007/978-3-030-87664-7_14">Chapter Link</a>]&nbsp;
                [<a href="./projects/DrF1/support/chapter.pdf">PDF</a>]&nbsp;
            </p>
          </div>
        </div>
        <br>
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <img src="images/DFC20_logo.png" width="300">
          </div>
          <div class="flex-item flex-item-stretch-4 flex-column">
            <p class="text">
                <a class="highlight-text" href="https://arxiv.org/abs/2102.09471">DeeperForensics Challenge 2020 on Real-World Face Forgery Detection: Methods and Results</a><br>
                <b>Liming Jiang</b>, Zhengkui Guo, Wayne Wu, Zhaoyang Liu, Ziwei Liu, Chen Change Loy, et al.<br>
                <i>arXiv preprint, 2021</i><br>
                [<a href="./projects/DFC20/resources/DFC20_Summary.pdf">Paper</a>]&nbsp;
                [<a href="https://github.com/EndlessSora/DeeperForensics-1.0">GitHub</a>&nbsp;<iframe src="https://img.shields.io/github/stars/EndlessSora/DeeperForensics-1.0?style=social" frameborder="0" vertical-align="middle" scrolling="0" width="90px" height="20px"></iframe>]&nbsp;
                [<a href="https://competitions.codalab.org/competitions/25228">Challenge Website</a>]
            </p>
          </div>
        </div>
        <br>
<!--        </tr>-->
<!--        <tr>-->
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <img src="images/VersatileI2I.png" width="300" height="162">
          </div>
          <div class="flex-item flex-item-stretch-4 flex-column">
            <p class="text">
                <a class="highlight-text" href="https://arxiv.org/abs/2007.12072">TSIT: A Simple and Versatile Framework for Image-to-Image Translation</a><br>
                <b>Liming Jiang</b>, Changxu Zhang, Mingyang Huang, Chunxiao Liu, Jianping Shi, Chen Change Loy<br>
                <i>European Conference on Computer Vision (ECCV), 2020 (<font color="#F00000">Spotlight</font>)</i><br>
                [<a href="./projects/TSIT/resources/paper.pdf">Paper</a>]&nbsp;
                [<a href="https://github.com/EndlessSora/TSIT">GitHub (Code)</a>&nbsp;<iframe src="https://img.shields.io/github/stars/EndlessSora/TSIT?style=social" frameborder="0" vertical-align="middle" scrolling="0" width="90px" height="20px"></iframe>]
            </p>
          </div>
        </div>
        <br>
<!--        </tr>-->
<!--        <tr>-->
        <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
            <img src="images/DeeperForensics1.png" width="300">
          </div>
          <div class="flex-item flex-item-stretch-4 flex-column">
            <p class="text">
                <a class="highlight-text" href="https://arxiv.org/abs/2001.03024">DeeperForensics-1.0: A Large-Scale Dataset for Real-World Face Forgery Detection</a><br>
                <b>Liming Jiang</b>, Ren Li, Wayne Wu, Chen Qian, Chen Change Loy<br>
                <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020</i><br>
                [<a href="./projects/DrF1/support/paper.pdf">Paper</a>]&nbsp;
                [<a href="./projects/DrF1/DrF1.html">Project Page</a>]&nbsp;
                [<a href="https://github.com/EndlessSora/DeeperForensics-1.0">GitHub (Dataset & Code)</a>&nbsp;<iframe src="https://img.shields.io/github/stars/EndlessSora/DeeperForensics-1.0?style=social" frameborder="0" vertical-align="middle" scrolling="0" width="90px" height="20px"></iframe>]
            </p>
          </div>
        </div>
<!--        </tr>-->
<!--        </table>-->
        <h2 id="AcademicEvents" class="add-top-margin">Academic Events</h2>
        <hr>
            <div>
                <li>Conference Reviewer: NeurIPS 2023, ICCV 2023, SIGGRAPH 2023, CVPR 2021-2023, ECCV 2022, AAAI 2022, NeurIPS 2021, ICCV 2021.</li>
                <li>Journal Reviewer: IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), International Journal of Computer Vision (IJCV), IEEE Transactions on Image Processing (TIP), IEEE Transactions on Multimedia (TMM), Computer Vision and Image Understanding (CVIU).</li>
                <li>Talk at <a href="https://eab.org/events/program/291">European Association for Biometrics (EAB) Workshop Digital Face Manipulation & Detection</a>.</li>
                <li>Organizer of <a href="https://www.mmlab-ntu.com/openmmlab/workshop_2021/">OpenMMLab Workshop 2021</a>.</li>
                <li>Talk at <a href="https://sense-human.github.io/index_2020.html">ECCV 2020 SenseHuman Workshop</a>: <i>DeeperForensics Challenge 2020 Briefing</i>. &nbsp;[<a href="https://www.youtube.com/watch?v=KzafMEIGoHg&list=PLua4XbbBXzFezSfmoZwgiIS5Ze0__mi70&index=6&t=0s">YouTube</a>]</li>
                <li>Organizer of <a href="https://competitions.codalab.org/competitions/25228">DeeperForensics Challenge 2020</a>.</li>
                <li>Organizer of <a href="https://sense-human.github.io/index_2020.html">ECCV 2020, The 2nd Workshop on Sensing, Understanding and Synthesizing Humans</a>.</li>
            </div>

        <h2 id="Awards" class="add-top-margin">Competitions</h2>
        <hr>
            <div>
                <li>ACM-ICPC Asia Regional Contest, <b>Gold Medal</b>, 2017</li>
                <li>MCM/ICM, <b>Meritorious Winner</b>, 2017</li>
                <li>China Collegiate Programming Contest (CCPC) of Northeast Area, <b>Gold Medal</b>, 2016</li>
                <li>The 31st Chinese Physics Olympiad, <b>First Prize</b>, 2014</li>
                <li>NOIP, <b>First Prize</b>, 2010</li>
            </div>

        <h2 id="HonorsandAwards" class="add-top-margin">Honors and Awards</h2>
        <hr>
            <div>
                <li>SDSC Dissertation Research Fellowship (<b>6</b> recipients in Singapore), Singapore Data Science Consortium, 2021</li>
                <li>NAP-SUG Research Scholarship, Nanyang Technological University, 2019</li>
                <li>National Scholarship (<b>top 1%</b>), Ministry of Education of China, 2016, 2018</li>
                <li>Excellent Student Award (every year), Jilin University, 2016, 2017, 2018</li>
                <li>CCF Outstanding Student Award (<b>72</b> students in China), China Computer Federation, 2018</li>
                <li>Qihoo 360 Scholarship, 2018</li>
                <li>SenseTime Scholarship (<b>24</b> students in China), 2017</li>
                <li>CASC Scholarship, 2017</li>
            </div>

        <h2 id="Teaching" class="add-top-margin">Teaching</h2>
        <hr>
          Teaching Assistant of the following courses at Nanyang Technological University (NTU):
            <div>
                <li>SC1005: <i>Digital Logic</i>, Fall 2021.</li>
                <li>AI6126: <i>Advanced Computer Vision</i>, Fall 2020.</li>
                <li>CE7491: <i>Special Advanced Topic: Digital Image Processing</i>, Fall 2020.</li>
                <li>CE/CZ4073: <i>Data Science for Business</i>, Spring 2020.</li>
                <li>CE/CZ1005: <i>Digital Logic</i>, Fall 2019.</li>
            </div>

        <br>
        <!--<hr>-->
        <br>
        <div id="clustrmaps-widget"></div>
        <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=5053a3&w=400&t=tt&d=EHW41rbaOsG22XAx1_kKGP2JHezOEeXFTFCEAn1NGe4&co=ffffff&ct=000000'></script>
      </div>
    </div>
  </div>
</body>

</html>
